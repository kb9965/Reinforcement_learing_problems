{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNM4lr9o5tZaeZ69NTlTDTe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Ownw80q-dxk","executionInfo":{"status":"ok","timestamp":1710991643659,"user_tz":-330,"elapsed":1668,"user":{"displayName":"KARTHIK B (RA2112701010014)","userId":"06219581223758280302"}},"outputId":"05f6bcb0-ff7d-40ed-d19a-c062eb5d5951"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal Value Function:\n","[[-29.25897645 -25.26209284 -24.64218742 -26.67726014 -31.52834723]\n"," [-28.65016382 -22.40854823 -20.58982039 -19.71595839 -25.64160392]\n"," [-26.82229635 -21.29126284 -18.98481763 -14.89171007 -12.52016425]\n"," [-32.56750966 -24.6544248  -19.2109392   -6.57347466   0.        ]]\n"]}],"source":["import numpy as np\n","\n","# Define grid dimensions\n","GRID_WIDTH = 5\n","GRID_HEIGHT = 4\n","\n","# Define rewards and transitions\n","REWARDS = {\n","    'goal': 10,\n","    'obstacle': -10,\n","    'step': -1\n","}\n","\n","# Define action space: up, down, left, right\n","ACTIONS = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n","\n","# Define parameters\n","alpha = 0.1  # Learning rate\n","gamma = 0.9  # Discount factor\n","epochs = 1000\n","\n","# Initialize value function\n","V = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n","\n","# Start TD(0) algorithm\n","for _ in range(epochs):\n","    # Initialize starting state\n","    state = (0, 0)  # Start position\n","\n","    while state != (GRID_HEIGHT - 1, GRID_WIDTH - 1):  # Continue until reaching the goal\n","        # Choose action\n","        action = np.random.choice(len(ACTIONS))  # Random action selection\n","\n","        # Take action and observe next state and reward\n","        next_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])\n","        next_state = (min(max(next_state[0], 0), GRID_HEIGHT - 1), min(max(next_state[1], 0), GRID_WIDTH - 1))  # Ensure next state is within grid boundaries\n","\n","        if next_state == (GRID_HEIGHT - 1, GRID_WIDTH - 1):  # Reached goal\n","            reward = REWARDS['goal']\n","        elif next_state == state:  # Hit obstacle\n","            reward = REWARDS['obstacle']\n","        else:\n","            reward = REWARDS['step']\n","\n","        # Update value function\n","        V[state[0], state[1]] += alpha * (reward + gamma * V[next_state[0], next_state[1]] - V[state[0], state[1]])\n","\n","        # Move to next state\n","        state = next_state\n","\n","# Print the estimated optimal value function\n","print(\"Optimal Value Function:\")\n","print(V)\n"]}]}